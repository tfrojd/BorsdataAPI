{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screeners\n",
    "Some examples on how you can create your own screeners and export txt-files that are importable in TradingView.\n",
    "This notebook utilizes the same pattern as the breadth-indicator one; big dataframe with unique identifiers into groupy -> apply for data manipulation and some pandas magic to get the data out.\n",
    "First we will create some helper functions to be able to create TradingView-compatible tickers, calculate some momentum parameters, earnings and revenue growth and turnover (SEK).\n",
    "These parameters will then be used in various way to generate importable text-files for TradingView.\n",
    "\n",
    "We will utilize stored pickles for the data and the dataframes can grow quite large hence it'll be kinda heavy on RAM.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchlist created: file_exports/abs65_top_100_turnover_greater_than_5e6.txt\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import csv\n",
    "import constants\n",
    "import pandas as pd \n",
    "import os\n",
    "\n",
    "\n",
    "# pandas options for string representation of data frames (print)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def create_trading_view_watchlist(df: pd.DataFrame, filename: str):\n",
    "     tv_tickers = []\n",
    "     for index, row in df.iterrows():\n",
    "          ticker = row.ticker.replace(\" \", \"_\")\n",
    "          market = row.market.lower()\n",
    "          country = row.country.lower()\n",
    "          ticker_tv = \"\"\n",
    "          if market == \"spotlight\" or market == \"ngm\":\n",
    "               ticker_tv = \"ngm:\" + ticker\n",
    "          elif country == \"sverige\":\n",
    "               ticker_tv = \"omxsto:\" + ticker\n",
    "          elif country == \"finland\":\n",
    "               ticker_tv = \"omxhex:\" + ticker\n",
    "          elif country == \"danmark\":\n",
    "               ticker_tv = \"omxcop:\" + ticker\n",
    "          elif country == \"norge\":\n",
    "               ticker_tv = \"osl:\" + ticker\n",
    "          tv_tickers.append(ticker_tv)\n",
    "     with open(constants.EXPORT_PATH + filename, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(tv_tickers)\n",
    "        print(f\"Watchlist created: {constants.EXPORT_PATH + filename}\")\n",
    "\n",
    "def calculate_momentum_and_turnover(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df[\"abs65\"] = df[\"close\"]/df[\"close\"].rolling(65).min()\n",
    "    df[\"abs125\"] = df[\"close\"]/df[\"close\"].rolling(125).min()\n",
    "    df[\"oneil_rs\"] = df[\"close\"].pct_change(65)*2 + df[\"close\"].pct_change(125) + df[\"close\"].pct_change(189) + df[\"close\"].pct_change(252)\n",
    "    df[\"turnover\"] = df[\"close\"]*df[\"volume\"] if df[\"country\"].values[0] != \"finland\" else df[\"close\"]*df[\"volume\"]*10 # simple conversion for EUR (skipping other currency diffs)\n",
    "    df[\"turnover_sma50\"] = df[\"turnover\"].rolling(50).mean()\n",
    "    return df\n",
    "\n",
    "# read stored stock data from pickle if it exists else create it\n",
    "if os.path.isfile(constants.EXPORT_PATH + \"data.pickle\"):\n",
    "     symbols = pd.read_pickle(constants.EXPORT_PATH + \"data.pickle\")\n",
    "else:\n",
    "     import borsdata_client as bc\n",
    "     client = bc.BorsdataClient()\n",
    "     symbols = client.get_all_stock_data_and_save_to_disk()\n",
    "# calculate momentum parameters for each df\n",
    "symbols = symbols.groupby(\"stock_id\", as_index=False).apply(lambda x: calculate_momentum_and_turnover(x))\n",
    "symbols.set_index(\"date\", inplace=True)\n",
    "# get top 100 stock sorted by absolute momentum 65 days and some liquidity filter and create a watchlist for TradingView.\n",
    "abs65 = symbols[(symbols.index == symbols.last_valid_index()) & (symbols[\"turnover_sma50\"] > 5e6)].sort_values(\"abs65\").tail(100).copy()\n",
    "create_trading_view_watchlist(abs65, \"abs65_top_100_turnover_greater_than_5e6.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report data\n",
    "\n",
    "So what if we wanted to get report data into the equation... My personal favorites are growth of earnings and revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_pct_change(end, start):\n",
    "    if end <= 0 or end == float(\"inf\") or start == float(\"inf\") or start == 0:\n",
    "        return np.nan\n",
    "    return (end - start) / abs(start) \n",
    "\n",
    "def calculate_growth_quarterly(reports_quarter: pd.DataFrame) -> pd.DataFrame:\n",
    "    reports_quarter.sort_values(\"report_date\", inplace=True)\n",
    "    reports_quarter['net_profit_margin'] = reports_quarter['profit_to_equity_holders'] / \\\n",
    "        reports_quarter['revenues']\n",
    "\n",
    "    for _index, row in reports_quarter.iterrows():\n",
    "        current_rev = row['revenues']\n",
    "        current_eps = row['earnings_per_share']\n",
    "        current_npm = row['net_profit_margin']\n",
    "        year = row['year']\n",
    "        period = row['period']\n",
    "        try:\n",
    "            prev_year = year - 1\n",
    "            prev_eps = reports_quarter.loc[(reports_quarter['year'] == prev_year) & (\n",
    "                reports_quarter['period'] == period)].earnings_per_share.values[0]\n",
    "            prev_rev = reports_quarter.loc[(reports_quarter['year'] == prev_year) & (\n",
    "                reports_quarter['period'] == period)].revenues.values[0]\n",
    "            prev_npm = reports_quarter.loc[(reports_quarter['year'] == prev_year) & (\n",
    "                reports_quarter['period'] == period)].net_profit_margin.values[0]\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'eps_growth_quarter_yy'] = calc_pct_change(\n",
    "                current_eps, prev_eps)\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'rev_growth_quarter_yy'] = calc_pct_change(\n",
    "                current_rev, prev_rev)\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'npm_growth_quarter_yy'] = calc_pct_change(\n",
    "                current_npm, prev_npm)\n",
    "\n",
    "        except Exception as e:\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'eps_growth_quarter_yy'] = np.nan\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'rev_growth_quarter_yy'] = np.nan\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'npm_growth_quarter_yy'] = np.nan\n",
    "    return reports_quarter\n",
    "            \n",
    "# read stored stock data from pickle if it exists else create it\n",
    "if os.path.isfile(constants.EXPORT_PATH + \"reports_quarter_data.pickle\"):\n",
    "    reports_quarter = pd.read_pickle(constants.EXPORT_PATH + \"reports_quarter_data.pickle\")\n",
    "else:\n",
    "    import borsdata_client as bc\n",
    "    client = bc.BorsdataClient()\n",
    "    client.get_all_report_data_and_save_to_disk()\n",
    "\n",
    "if not os.path.isfile(constants.EXPORT_PATH + \"reports_quarter_growth.pickle\"):\n",
    "    reports_quarter = pd.read_pickle(constants.EXPORT_PATH + \"reports_quarter_data.pickle\")\n",
    "    reports_quarter[\"report_date\"] = pd.to_datetime(reports_quarter[\"report_date\"])\n",
    "    # calculate growth parameters for the report data\n",
    "    reports_quarter = reports_quarter.groupby(\"stock_id\", as_index=False).apply(lambda x: calculate_growth_quarterly(x))\n",
    "    # store it if we want to pick it up later (the calculation is a bit heavy and takes ~5-6 minutes)\n",
    "    reports_quarter.to_pickle(constants.EXPORT_PATH + \"reports_quarter_growth.pickle\")\n",
    "else:\n",
    "    reports_quarter = pd.read_pickle(constants.EXPORT_PATH + \"reports_quarter_growth.pickle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have price data and report data available. It is time to combine it to a new dataframe. We will loop through all the values for the last valid index in the price data and fetch the report data that we are interested in and combine it into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name ticker     market country     abs65    abs125   \n",
      "1656   Sparebanken Vest   SVEG  Oslo Bors   Norge  1.079812  1.117861  \\\n",
      "1657  Sandnes Sparebank   SADG  Oslo Bors   Norge  1.000000  1.000000   \n",
      "1658   Totens Sparebank   TOTG  Oslo Bors   Norge  1.029703  1.142857   \n",
      "1659    Sparebanken Sor    SOR  Oslo Bors   Norge  1.015873  1.053498   \n",
      "1660  Aurskog Sparebank   AURG  Oslo Bors   Norge  1.009524  1.019231   \n",
      "\n",
      "      oneil_rs  eps_growth_q1_yy  eps_growth_q2_yy  rev_growth_q1_yy   \n",
      "1656  0.086402         -0.106697          0.341398          0.030984  \\\n",
      "1657 -0.381570         -0.332360         -0.182476          0.031264   \n",
      "1658 -0.171965          0.118238          0.063964          0.137108   \n",
      "1659 -0.184222          0.429874         -0.152867          0.307918   \n",
      "1660  0.045719          1.148324          0.145225          0.340136   \n",
      "\n",
      "      rev_growth_q2_yy  turnover_sma50  \n",
      "1656          0.276687     8963981.282  \n",
      "1657         -0.024072      794880.136  \n",
      "1658          0.111194      102256.160  \n",
      "1659          0.028919      580906.520  \n",
      "1660          0.251321      476904.040  \n"
     ]
    }
   ],
   "source": [
    "stock_dict_list = []\n",
    "for index, row in symbols[(symbols.index == symbols.last_valid_index())].iterrows():\n",
    "    stock_id = row[\"stock_id\"]\n",
    "    try:\n",
    "        # get the reports for this stock\n",
    "        reports = reports_quarter[reports_quarter.stock_id == stock_id]\n",
    "        # last value == latest, easiest way to get it is by indexing \"backwards\" -1 represents the last element\n",
    "        # eps data\n",
    "        last_reported_eps_growth = reports[\"eps_growth_quarter_yy\"].values[-1] \n",
    "        second_last_reported_eps_growth = reports[\"eps_growth_quarter_yy\"].values[-2]\n",
    "        # revenue data\n",
    "        last_reported_rev_growth = reports[\"rev_growth_quarter_yy\"].values[-1] \n",
    "        second_last_reported_rev_growth = reports[\"rev_growth_quarter_yy\"].values[-2]\n",
    "        # append a new dict-object to the list\n",
    "        stock_dict_list.append({\"name\": row[\"name\"],\"ticker\": row[\"ticker\"], \"market\": row[\"market\"], \"country\": row[\"country\"], \n",
    "                                \"abs65\": row[\"abs65\"], \"abs125\": row[\"abs125\"], \"oneil_rs\": row[\"oneil_rs\"], \"eps_growth_q1_yy\": last_reported_eps_growth, \n",
    "                                \"eps_growth_q2_yy\": second_last_reported_eps_growth, \"rev_growth_q1_yy\": last_reported_rev_growth,\n",
    "                                \"rev_growth_q2_yy\": second_last_reported_rev_growth, \"turnover_sma50\": row[\"turnover_sma50\"]})\n",
    "    except Exception as e:\n",
    "        # for some companies the data won't be available. Just pass the error and get to the next.\n",
    "        pass\n",
    "\n",
    "# create a the new dataframe from the list of dictionaries.\n",
    "momo_and_growth = pd.DataFrame(stock_dict_list)\n",
    "# print the result\n",
    "print(momo_and_growth.tail())\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe containing both the price data and growth parameters for the last two reported quarters for earnings and revenues. Lets create some new watchlists (screeners) from the newly created data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchlist created: file_exports/abs65_top50_and_eps_and_rev_growth.txt\n"
     ]
    }
   ],
   "source": [
    "# eps and revenues growth > 0% last quarter and turnover last 50 day average greater than 5 million SEK, sort by abs65 rank and get top50.\n",
    "temp = momo_and_growth[(momo_and_growth[\"eps_growth_q1_yy\"] > 0) & (momo_and_growth[\"rev_growth_q1_yy\"] > 0) & (momo_and_growth[\"turnover_sma50\"] > 5e6)].sort_values(\"abs65\").tail(50)\n",
    "create_trading_view_watchlist(temp, \"abs65_top50_and_eps_and_rev_growth.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quite common approach while screening is ranking. Lets rank on the momentum attributes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchlist created: file_exports/abs65_or_abs125_rank_greater_than_90_and_eps_and_rev_growth.txt\n",
      "Watchlist created: file_exports/rs_rank_greater_than_90_and_eps_and_rev_growth.txt\n"
     ]
    }
   ],
   "source": [
    "momo_and_growth[\"abs65_rank\"] = momo_and_growth[\"abs65\"].rank(ascending=True, pct=True).round(2) * 100\n",
    "momo_and_growth[\"abs125_rank\"] = momo_and_growth[\"abs125\"].rank(ascending=True, pct=True).round(2) * 100\n",
    "# this is the infamoues \"RS-Rank\"\n",
    "momo_and_growth[\"rs\"] = momo_and_growth[\"oneil_rs\"].rank(ascending=True, pct=True).round(2) * 100\n",
    "\n",
    "# eps and revenues growth > 0% last quarter and turnover last 50 day average greater than 5 million SEK and abs65-rank or abs125-rank greater than 90 (top 10% perfomer)\n",
    "temp = momo_and_growth[(momo_and_growth[\"eps_growth_q1_yy\"] > 0) & (momo_and_growth[\"rev_growth_q1_yy\"] > 0) & (momo_and_growth[\"turnover_sma50\"] > 5e6) & ((momo_and_growth[\"abs65_rank\"] > 90) | ((momo_and_growth[\"abs125_rank\"] > 90)))]\n",
    "create_trading_view_watchlist(temp, \"abs65_or_abs125_rank_greater_than_90_and_eps_and_rev_growth.txt\")\n",
    "\n",
    "temp = momo_and_growth[(momo_and_growth[\"eps_growth_q1_yy\"] > 0) & (momo_and_growth[\"rev_growth_q1_yy\"] > 0) & (momo_and_growth[\"rs\"] > 90) & (momo_and_growth[\"turnover_sma50\"] > 5e6)]\n",
    "create_trading_view_watchlist(temp, \"rs_rank_greater_than_90_and_eps_and_rev_growth.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was all, I hope you'll find it useful and educative. If any questions arise feel free to contact me on [Twitter](https://twitter.com/TapeReaderJoe).\n",
    "\n",
    "And of course there might be some errors in these calculations and the code can hopefully been written nicer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
