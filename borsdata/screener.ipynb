{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Screeners\n",
    "Some examples on how you can create your own screeners and export txt-files that are importable in TradingView.\n",
    "This notebook utilizes the same pattern as the breadth-indicator one; big dataframe with unique identifiers into groupy -> apply for data manipulation and some pandas magic to get the data out.\n",
    "First we will create some helper functions to be able to create TradingView-compatible tickers, calculate some momentum parameters, earnings and revenue growth and turnover (SEK).\n",
    "These parameters will then be used in various way to generate importable text-files for TradingView.\n",
    "\n",
    "We will utilize stored pickles for the data and the dataframes can grow quite large hence it'll be kinda heavy on RAM.\n",
    "\n",
    "Let's go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchlist created: file_exports/abs65_top_100_turnover_greater_than_5e6.txt\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import csv\n",
    "import constants\n",
    "import pandas as pd \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# pandas options for string representation of data frames (print)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "def create_trading_view_watchlist(df: pd.DataFrame, filename: str):\n",
    "     tv_tickers = []\n",
    "     for index, row in df.iterrows():\n",
    "          ticker = row.ticker.replace(\" \", \"_\")\n",
    "          market = row.market.lower()\n",
    "          country = row.country.lower()\n",
    "          ticker_tv = \"\"\n",
    "          if market == \"spotlight\" or market == \"ngm\":\n",
    "               ticker_tv = \"ngm:\" + ticker\n",
    "          elif country == \"sverige\":\n",
    "               ticker_tv = \"omxsto:\" + ticker\n",
    "          elif country == \"finland\":\n",
    "               ticker_tv = \"omxhex:\" + ticker\n",
    "          elif country == \"danmark\":\n",
    "               ticker_tv = \"omxcop:\" + ticker\n",
    "          elif country == \"norge\":\n",
    "               ticker_tv = \"osl:\" + ticker\n",
    "          tv_tickers.append(ticker_tv)\n",
    "     with open(constants.EXPORT_PATH + filename, 'w') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(tv_tickers)\n",
    "        print(f\"Watchlist created: {constants.EXPORT_PATH + filename}\")\n",
    "\n",
    "def calculate_momentum_and_turnover(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    # net up daus is also a fun and seldomly used momentum indicator.\n",
    "    df[\"net_up_days_252\"] = (df['close'].pct_change() >= 0).rolling(252).sum()\n",
    "    df[\"abs65\"] = df[\"close\"]/df[\"close\"].rolling(65).min()\n",
    "    df[\"abs125\"] = df[\"close\"]/df[\"close\"].rolling(125).min()\n",
    "    df[\"oneil_rs\"] = df[\"close\"].pct_change(65)*2 + df[\"close\"].pct_change(125) + df[\"close\"].pct_change(189) + df[\"close\"].pct_change(252)\n",
    "    df[\"turnover\"] = df[\"close\"]*df[\"volume\"] if df[\"country\"].values[0] != \"finland\" else df[\"close\"]*df[\"volume\"]*10 # simple conversion for EUR (skipping other currency diffs)\n",
    "    df[\"turnover_sma50\"] = df[\"turnover\"].rolling(50).mean()\n",
    "    return df\n",
    "\n",
    "# read stored stock data from pickle if it exists else create it\n",
    "if os.path.isfile(constants.EXPORT_PATH + \"data.pickle\"):\n",
    "     symbols = pd.read_pickle(constants.EXPORT_PATH + \"data.pickle\")\n",
    "else:\n",
    "     import borsdata_client as bc\n",
    "     client = bc.BorsdataClient()\n",
    "     symbols = client.get_all_stock_data_and_save_to_disk()\n",
    "# calculate momentum parameters for each df\n",
    "symbols = symbols.groupby(\"stock_id\", as_index=False).apply(lambda x: calculate_momentum_and_turnover(x))\n",
    "symbols.set_index(\"date\", inplace=True)\n",
    "# get top 100 stock sorted by absolute momentum 65 days and some liquidity filter and create a watchlist for TradingView.\n",
    "abs65 = symbols[(symbols.index == symbols.last_valid_index()) & (symbols[\"turnover_sma50\"] > 5e6)].sort_values(\"abs65\").tail(100).copy()\n",
    "create_trading_view_watchlist(abs65, \"abs65_top_100_turnover_greater_than_5e6.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report data\n",
    "\n",
    "So what if we wanted to get report data into the equation... My personal favorites are growth of earnings and revenues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def calc_pct_change(end, start):\n",
    "    if end <= 0 or end == float(\"inf\") or start == float(\"inf\") or start == 0:\n",
    "        return np.nan\n",
    "    return (end - start) / abs(start)\n",
    "\n",
    "def calc_pct_change_series(column, shift):\n",
    "    ret_val = np.nan\n",
    "    try:\n",
    "        ret_val = column.pct_change(shift) * np.sign(column.shift(shift))\n",
    "    except:\n",
    "        pass\n",
    "    return ret_val\n",
    "\n",
    "def calculate_growth_quarterly(reports_quarter: pd.DataFrame) -> pd.DataFrame:\n",
    "    reports_quarter.sort_values(\"report_date\", inplace=True)\n",
    "    reports_quarter['net_profit_margin'] = reports_quarter['profit_to_equity_holders'] / \\\n",
    "        reports_quarter['revenues']\n",
    "\n",
    "    for _index, row in reports_quarter.iterrows():\n",
    "        current_rev = row['revenues']\n",
    "        current_eps = row['earnings_per_share']\n",
    "        current_npm = row['net_profit_margin']\n",
    "        year = row['year']\n",
    "        period = row['period']\n",
    "        try:\n",
    "            prev_year = year - 1\n",
    "            prev_eps = reports_quarter.loc[(reports_quarter['year'] == prev_year) & (\n",
    "                reports_quarter['period'] == period)].earnings_per_share.values[0]\n",
    "            prev_rev = reports_quarter.loc[(reports_quarter['year'] == prev_year) & (\n",
    "                reports_quarter['period'] == period)].revenues.values[0]\n",
    "            prev_npm = reports_quarter.loc[(reports_quarter['year'] == prev_year) & (\n",
    "                reports_quarter['period'] == period)].net_profit_margin.values[0]\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'eps_growth_quarter_yy'] = calc_pct_change(\n",
    "                current_eps, prev_eps)\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'rev_growth_quarter_yy'] = calc_pct_change(\n",
    "                current_rev, prev_rev)\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'npm_growth_quarter_yy'] = calc_pct_change(\n",
    "                current_npm, prev_npm)\n",
    "\n",
    "        except Exception as e:\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'eps_growth_quarter_yy'] = np.nan\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'rev_growth_quarter_yy'] = np.nan\n",
    "            reports_quarter.loc[(reports_quarter['year'] == year) & (\n",
    "                reports_quarter['period'] == period), 'npm_growth_quarter_yy'] = np.nan\n",
    "    return reports_quarter\n",
    "\n",
    "def calculate_growth_yearly(reports_year):\n",
    "    reports_year.sort_values(\"report_date\", inplace=True)\n",
    "    reports_year['eps_growth1'] = calc_pct_change_series(\n",
    "        reports_year['earnings_per_share'], 1)\n",
    "    reports_year['eps_growth2'] = calc_pct_change_series(\n",
    "        reports_year['earnings_per_share'], 2)\n",
    "    reports_year['eps_growth3'] = calc_pct_change_series(\n",
    "        reports_year['earnings_per_share'], 3)\n",
    "    reports_year['rev_growth1'] = calc_pct_change_series(\n",
    "        reports_year['revenues'], 1)\n",
    "    reports_year['rev_growth2'] = calc_pct_change_series(\n",
    "        reports_year['revenues'], 2)\n",
    "    reports_year['rev_growth3'] = calc_pct_change_series(\n",
    "        reports_year['revenues'], 3)\n",
    "    reports_year['net_profit_margin'] = reports_year['profit_to_equity_holders'] / \\\n",
    "        reports_year['revenues']\n",
    "    return reports_year\n",
    "            \n",
    "# read stored stock data from pickle if it exists else create it\n",
    "if os.path.isfile(constants.EXPORT_PATH + \"reports_quarter_data.pickle\") and os.path.isfile(constants.EXPORT_PATH + \"reports_year_data.pickle\") :\n",
    "    reports_quarter = pd.read_pickle(constants.EXPORT_PATH + \"reports_quarter_data.pickle\")\n",
    "    reports_year = pd.read_pickle(constants.EXPORT_PATH + \"reports_year_data.pickle\")\n",
    "else:\n",
    "    import borsdata_client as bc\n",
    "    client = bc.BorsdataClient()\n",
    "    client.get_all_report_data_and_save_to_disk()\n",
    "\n",
    "if not os.path.isfile(constants.EXPORT_PATH + \"reports_quarter_growth.pickle\"):\n",
    "    reports_quarter = pd.read_pickle(constants.EXPORT_PATH + \"reports_quarter_data.pickle\")\n",
    "    reports_quarter[\"report_date\"] = pd.to_datetime(reports_quarter[\"report_date\"])\n",
    "    # calculate growth parameters for the report data\n",
    "    reports_quarter = reports_quarter.groupby(\"stock_id\", as_index=False).apply(lambda x: calculate_growth_quarterly(x))\n",
    "    # store it if we want to pick it up later (the calculation is a bit heavy and takes ~5-6 minutes)\n",
    "    reports_quarter.to_pickle(constants.EXPORT_PATH + \"reports_quarter_growth.pickle\")\n",
    "else:\n",
    "    reports_quarter = pd.read_pickle(constants.EXPORT_PATH + \"reports_quarter_growth.pickle\")\n",
    "\n",
    "if not os.path.isfile(constants.EXPORT_PATH + \"reports_year_growth.pickle\"):\n",
    "    reports_year = pd.read_pickle(constants.EXPORT_PATH + \"reports_year_data.pickle\")\n",
    "    reports_year[\"report_date\"] = pd.to_datetime(reports_year[\"report_date\"])\n",
    "    # calculate growth parameters for the report data\n",
    "    reports_year = reports_year.groupby(\"stock_id\", as_index=False).apply(lambda x: calculate_growth_yearly(x))\n",
    "    # store it if we want to pick it up later (the calculation is a bit heavy and takes ~5-6 minutes)\n",
    "    reports_year.to_pickle(constants.EXPORT_PATH + \"reports_year_growth.pickle\")\n",
    "else:\n",
    "    reports_year = pd.read_pickle(constants.EXPORT_PATH + \"reports_year_growth.pickle\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have price data and report data available. It is time to combine it to a new dataframe. We will loop through all the values for the last valid index in the price data and fetch the report data that we are interested in and combine it into a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   name ticker     market country     abs65    abs125   \n",
      "1659   Sparebanken Vest   SVEG  Oslo Bors   Norge  1.077465  1.115431  \\\n",
      "1660  Sandnes Sparebank   SADG  Oslo Bors   Norge  1.009238  1.009238   \n",
      "1661   Totens Sparebank   TOTG  Oslo Bors   Norge  1.019802  1.131868   \n",
      "1662    Sparebanken Sor    SOR  Oslo Bors   Norge  1.000000  1.024691   \n",
      "1663  Aurskog Sparebank   AURG  Oslo Bors   Norge  1.000000  1.009615   \n",
      "\n",
      "      oneil_rs  net_up_days_252  eps_growth_q1_yy  eps_growth_q2_yy   \n",
      "1659  0.065752            132.0         -0.106697          0.341398  \\\n",
      "1660 -0.344683            133.0         -0.332360         -0.182476   \n",
      "1661 -0.167826            164.0          0.118238          0.063964   \n",
      "1662 -0.370788            140.0          0.429874         -0.152867   \n",
      "1663 -0.001882            188.0          1.148324          0.145225   \n",
      "\n",
      "      eps_growth_1y  rev_growth_q1_yy  rev_growth_q2_yy  rev_growth_1y   \n",
      "1659       0.244928          0.030984          0.276687       0.127781  \\\n",
      "1660       0.078740          0.031264         -0.024072       0.009547   \n",
      "1661       0.164194          0.137108          0.111194       0.082484   \n",
      "1662       0.117916          0.307918          0.028919       0.042962   \n",
      "1663       0.150097          0.340136          0.251321       0.123228   \n",
      "\n",
      "      turnover_sma50  \n",
      "1659     8997990.632  \n",
      "1660      799450.320  \n",
      "1661      102068.840  \n",
      "1662      583820.850  \n",
      "1663      475671.440  \n"
     ]
    }
   ],
   "source": [
    "stock_dict_list = []\n",
    "for index, row in symbols[(symbols.index == symbols.last_valid_index())].iterrows():\n",
    "    stock_id = row[\"stock_id\"]\n",
    "    try:\n",
    "        # get the reports for this stock\n",
    "        reports_q = reports_quarter[reports_quarter.stock_id == stock_id]\n",
    "        reports_y = reports_year[reports_year.stock_id == stock_id]\n",
    "        # last value == latest, easiest way to get it is by indexing \"backwards\" -1 represents the last element\n",
    "        # -2 the second last etc.\n",
    "        # eps data\n",
    "        last_reported_eps_growth = reports_q[\"eps_growth_quarter_yy\"].values[-1] \n",
    "        second_last_reported_eps_growth = reports_q[\"eps_growth_quarter_yy\"].values[-2]\n",
    "        last_reported_eps_growth_y = reports_y[\"eps_growth1\"].values[-1]\n",
    "        # revenue data\n",
    "        last_reported_rev_growth = reports_q[\"rev_growth_quarter_yy\"].values[-1] \n",
    "        second_last_reported_rev_growth = reports_q[\"rev_growth_quarter_yy\"].values[-2]\n",
    "        last_reported_rev_growth_y = reports_y[\"rev_growth1\"].values[-1]\n",
    "        # append a new dict-object to the list\n",
    "        stock_dict_list.append({\"name\": row[\"name\"],\"ticker\": row[\"ticker\"], \"market\": row[\"market\"], \"country\": row[\"country\"], \n",
    "                                \"abs65\": row[\"abs65\"], \"abs125\": row[\"abs125\"], \"oneil_rs\": row[\"oneil_rs\"], \"net_up_days_252\": row[\"net_up_days_252\"],\n",
    "                                \"eps_growth_q1_yy\": last_reported_eps_growth, \n",
    "                                \"eps_growth_q2_yy\": second_last_reported_eps_growth,\n",
    "                                \"eps_growth_1y\": last_reported_eps_growth_y, \n",
    "                                \"rev_growth_q1_yy\": last_reported_rev_growth,\n",
    "                                \"rev_growth_q2_yy\": second_last_reported_rev_growth,\n",
    "                                \"rev_growth_1y\": last_reported_rev_growth_y,\n",
    "                                \"turnover_sma50\": row[\"turnover_sma50\"]})\n",
    "    except Exception as e:\n",
    "        # for some companies the data won't be available. Just pass the error and get to the next.\n",
    "        pass\n",
    "\n",
    "# create a the new dataframe from the list of dictionaries.\n",
    "momo_and_growth = pd.DataFrame(stock_dict_list)\n",
    "# print the result\n",
    "print(momo_and_growth.tail())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a dataframe containing both the price data and growth parameters for the last two reported quarters and the last year for earnings and revenues. Lets create some new watchlists (screeners) from the newly created data and export the dataframe to excel for all the excel-crunchers out there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchlist created: file_exports/eps_and_rev_growth.txt\n"
     ]
    }
   ],
   "source": [
    "# save to excel\n",
    "momo_and_growth.to_excel(constants.EXPORT_PATH + \"momo_and_growth.xlsx\")\n",
    "# eps and revenues growth > 0% last two quarters and last year, turnover last 50 day average greater than 5 million SEK\n",
    "temp = momo_and_growth[(momo_and_growth[\"eps_growth_q1_yy\"] > 0) &\n",
    "                       (momo_and_growth[\"eps_growth_q2_yy\"] > 0) &\n",
    "                       (momo_and_growth[\"rev_growth_q1_yy\"] > 0) & \n",
    "                       (momo_and_growth[\"rev_growth_q2_yy\"] > 0) & \n",
    "                       (momo_and_growth[\"eps_growth_1y\"] > 0) & \n",
    "                       (momo_and_growth[\"rev_growth_1y\"] > 0) & \n",
    "                       (momo_and_growth[\"turnover_sma50\"] > 5e6)]\n",
    "create_trading_view_watchlist(temp, \"eps_and_rev_growth.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another quite common approach while screening is ranking. Lets rank on the momentum attributes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watchlist created: file_exports/abs65_or_abs125_rank_greater_than_90_and_eps_and_rev_growth.txt\n",
      "Watchlist created: file_exports/rs_rank_greater_than_90_and_eps_and_rev_growth.txt\n"
     ]
    }
   ],
   "source": [
    "momo_and_growth[\"abs65_rank\"] = momo_and_growth[\"abs65\"].rank(ascending=True, pct=True).round(2) * 100\n",
    "momo_and_growth[\"abs125_rank\"] = momo_and_growth[\"abs125\"].rank(ascending=True, pct=True).round(2) * 100\n",
    "# this is the infamous \"RS-Rank\"\n",
    "momo_and_growth[\"rs\"] = momo_and_growth[\"oneil_rs\"].rank(ascending=True, pct=True).round(2) * 100\n",
    "\n",
    "# eps and revenues growth > 0% last quarter and turnover last 50 day average greater than 5 million SEK and abs65-rank or abs125-rank greater than 90 (top 10% perfomer)\n",
    "temp = momo_and_growth[(momo_and_growth[\"eps_growth_q1_yy\"] > 0) & (momo_and_growth[\"rev_growth_q1_yy\"] > 0) & (momo_and_growth[\"turnover_sma50\"] > 5e6) & ((momo_and_growth[\"abs65_rank\"] > 90) | ((momo_and_growth[\"abs125_rank\"] > 90)))]\n",
    "create_trading_view_watchlist(temp, \"abs65_or_abs125_rank_greater_than_90_and_eps_and_rev_growth.txt\")\n",
    "\n",
    "temp = momo_and_growth[(momo_and_growth[\"eps_growth_q1_yy\"] > 0) & (momo_and_growth[\"rev_growth_q1_yy\"] > 0) & (momo_and_growth[\"rs\"] > 90) & (momo_and_growth[\"turnover_sma50\"] > 5e6)]\n",
    "create_trading_view_watchlist(temp, \"rs_rank_greater_than_90_and_eps_and_rev_growth.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was all, I hope you'll find it useful and educative. If any questions arise feel free to contact me on [Twitter](https://twitter.com/TapeReaderJoe).\n",
    "\n",
    "And of course there might be some errors in these calculations and the code can hopefully been written nicer!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
